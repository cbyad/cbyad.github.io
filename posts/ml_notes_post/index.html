<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">

    <meta name="author" content="Yannick-Alain Couassi-Blé D.">
    <meta name="description" content="Supervised Learning Linear regression with one variable Target : From a dataset (red points), we want to find a general method to automatically get an hypothesis function ${h_\theta(x)=\theta_0&#43;\theta_1x}$ (blue line) that fit as well our points to predict new incomming data. Solution : Find a cost function $J(\theta)$ of parameter $(\theta_0,\theta_1)$ with $\theta$ that minimise $J$
$$ J(\theta_0,\theta_1) = \dfrac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2 $$
where $m$ the number of training sets">
    <meta name="keywords" content="blog,developer,personal">

    
      <script src="https://twemoji.maxcdn.com/v/latest/twemoji.min.js" crossorigin="anonymous"></script>
    

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Machine Learning Notes"/>
<meta name="twitter:description" content="Supervised Learning Linear regression with one variable Target : From a dataset (red points), we want to find a general method to automatically get an hypothesis function ${h_\theta(x)=\theta_0&#43;\theta_1x}$ (blue line) that fit as well our points to predict new incomming data. Solution : Find a cost function $J(\theta)$ of parameter $(\theta_0,\theta_1)$ with $\theta$ that minimise $J$
$$ J(\theta_0,\theta_1) = \dfrac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2 $$
where $m$ the number of training sets"/>

    <meta property="og:title" content="Machine Learning Notes" />
<meta property="og:description" content="Supervised Learning Linear regression with one variable Target : From a dataset (red points), we want to find a general method to automatically get an hypothesis function ${h_\theta(x)=\theta_0&#43;\theta_1x}$ (blue line) that fit as well our points to predict new incomming data. Solution : Find a cost function $J(\theta)$ of parameter $(\theta_0,\theta_1)$ with $\theta$ that minimise $J$
$$ J(\theta_0,\theta_1) = \dfrac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2 $$
where $m$ the number of training sets" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://cbyad.github.io/posts/ml_notes_post/" />
<meta property="article:published_time" content="2020-09-27T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-09-27T00:00:00+00:00" />


    
      <base href="https://cbyad.github.io/posts/ml_notes_post/">
    
    <title>
  Machine Learning Notes · Cbyad
</title>

    
      <link rel="canonical" href="https://cbyad.github.io/posts/ml_notes_post/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.13.0/css/all.css" integrity="sha384-Bfad6CLCknfcloXFOyFnlgtENryhrpZCe29RTifKEixXQZ38WheV+i/6YWSzkz3V" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="https://cbyad.github.io/css/coder.min.3219ef62ae52679b7a9c19043171c3cd9f523628c2a65f3ef247ee18836bc90b.css" integrity="sha256-MhnvYq5SZ5t6nBkEMXHDzZ9SNijCpl8&#43;8kfuGINryQs=" crossorigin="anonymous" media="screen" />
    

    

    

    

    

    <link rel="icon" type="image/png" href="https://cbyad.github.io/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="https://cbyad.github.io/images/favicon-16x16.png" sizes="16x16">

    <meta name="generator" content="Hugo 0.72.0" />
  </head>

  
  
  <body class="colorscheme-light"
        onload=" twemoji.parse(document.body); "
  >
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="https://cbyad.github.io/">
      Cbyad
    </a>
    
    <input type="checkbox" id="menu-toggle" />
    <label class="menu-button float-right" for="menu-toggle"><i class="fas fa-bars"></i></label>
    <ul class="navigation-list">
      
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://cbyad.github.io/about/">About</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://cbyad.github.io/posts/">Posts</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://cbyad.github.io/projects/">Projects</a>
          </li>
        
      
      
    </ul>
    
  </section>
</nav>


      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">Machine Learning Notes</h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fas fa-calendar"></i>
              <time datetime='2020-09-27T00:00:00Z'>
                September 27, 2020
              </time>
            </span>
            <span class="reading-time">
              <i class="fas fa-clock"></i>
              2-minute read
            </span>
          </div>
          
          <div class="tags">
  <i class="fas fa-tag"></i>
    <a href="https://cbyad.github.io/tags/supervised-learning/">supervised learning</a>
      <span class="separator">•</span>
    <a href="https://cbyad.github.io/tags/unsupervised-learning/">unsupervised learning</a></div>

        </div>
      </header>

      <div>
        
        <h1 id="supervised-learning">Supervised Learning</h1>
<h2 id="linear-regression-with-one-variable">Linear regression with one variable</h2>
<p><strong>Target</strong> : From a dataset (red points), we want to find a general method to automatically get an <strong>hypothesis function</strong> ${h_\theta(x)=\theta_0+\theta_1x}$ (blue line) that fit as well our points to predict new incomming data.
<img src="https://cbyad.github.io/images/ml/goal.png" alt=""></p>
<p><strong>Solution</strong> : Find a <strong>cost function</strong> $J(\theta)$ of parameter $(\theta_0,\theta_1)$ with $\theta$ that minimise $J$</p>
<p>$$
J(\theta_0,\theta_1)  = \dfrac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2
$$</p>
<p>where $m$ the number of training sets</p>
<p><strong>Gradient descent</strong> algorithm to automatically compute and find $\theta$</p>
<p>$$
\theta_j := \theta_j - \alpha \dfrac{\partial}{\partial \theta_j} J(\theta_j), j=[0,1]
$$</p>
<p><strong>Warning</strong> : All $\theta_j$ are simultaneously updated until reach convergence</p>
<h2 id="multivariate-linear-regression">Multivariate linear regression</h2>
<p>We hold more than one variable. Hence our <strong>hypothesis function</strong> is now :
${h_\theta(x)=\theta_0+\theta_1x_1+\dots+\theta_nx_n}$</p>
<p>To simpplify expression we define $x_0=1$, $\theta$ is a $(n+1)$ dimensional vector</p>
<p>Vectorization of our hypothesis $\rightarrow$ ${h_\theta(X)=\theta^TX}$</p>
<p><strong>Cost function</strong> still the same</p>
<p>$$J(\theta) = \dfrac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)} )^2$$</p>
<p><strong>Gradient descent</strong> : Method to solve for $\theta$ iteratively</p>
<p>General form expression. $n$ the number of features in training sets
$$
\theta_j := \theta_j - \alpha \dfrac{\partial}{\partial \theta_j} J(\theta_j), j=[0, \dots,n]
$$</p>
<ul>
<li>Simplification</li>
</ul>
<p>When $j=0$ :
$$
\theta_0 := \theta_0 - \alpha \dfrac{\partial}{\partial \theta_0} [\dfrac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)} )^2]
$$
$$
\theta_0 := \theta_0 -\dfrac{\alpha}{\cancel{2}m}*\cancel{2} \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}
$$</p>
<p>We repeat this process when $j=1$, $j=2$, $\ldots$ More generally by induction we finally get :</p>
<p>$$
\theta_j := \theta_j -\dfrac{\alpha}{m}\sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
$$</p>
<ul>
<li>Optimisation : Feature scaling &amp; Mean normalisation</li>
</ul>
<p>When difference between values of features are too large, they cause that gradient descent run slowly to find the global optimum $\theta$. Instead of taking them as they are, we make a transformation to hold them at the same scale. <strong>This ensure that gradient descent will converge more faster</strong>.
From training set : $(a_i,b_i,\dots,y_i)$</p>
<p>$a_i \leftarrow \frac{a_i - \mu_i}{S_i}$
where $\mu_i$ is the mean of $a_i$ other the set and $S_i$ the standard deviation or simply range $(max_{value} - min_{value})$.</p>
<ul>
<li>Learning rate</li>
</ul>
<table>
<thead>
<tr>
<th align="center">_</th>
<th align="center">_</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><img src="https://cbyad.github.io/images/ml/learning_rates_1.png" alt=""></td>
<td align="center"><img src="https://cbyad.github.io/images/ml/learning_rates_2.png" alt=""></td>
</tr>
</tbody>
</table>
<p><strong>Normal Equation</strong> : Method to solve for $\theta$ analytically
$$
\theta = (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top y
$$
Where $X$ is the design matrix</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>$$ \mathbf{X} = \left(
\begin{array}{ccc} &mdash; &amp; [x^{(0)}]^\top  &amp; &mdash; \\ &mdash; &amp; [x^{(1)}]^\top &amp; &mdash; \\ &mdash; &amp; \ldots &amp; &mdash; \\ &mdash; &amp; [x^{(m)}]^\top &amp; &mdash;  \\ \end{array} \right) $$</p>
<ul>
<li>GDA vs NE</li>
</ul>
<table>
<thead>
<tr>
<th align="center">Gradient Descent</th>
<th align="center">Normal Equation</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Need to choose $\alpha$</td>
<td align="center">No need to choose $\alpha$</td>
</tr>
<tr>
<td align="center">Needs many iterations</td>
<td align="center">No needs many iterations</td>
</tr>
<tr>
<td align="center">$O(kn^2)$</td>
<td align="center">$O(n^3)$, need to calculate $(\mathbf{X}^\top \mathbf{X})^{-1}$</td>
</tr>
<tr>
<td align="center">Works well when n is large</td>
<td align="center">Slow if n is very large ($n&gt;10^4$)</td>
</tr>
</tbody>
</table>
<h2 id="logistic-regression">Logistic Regression</h2>
<p><strong>WIP&hellip;</strong></p>

      </div>


      <footer>
        


        <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "yourdiscussshortname" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
        
        
      </footer>
    </article>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.css" integrity="sha384-dbVIfZGuN1Yq7/1Ocstc1lUEm+AT+/rCkibIcC/OmWo5f0EA48Vf8CytHzGrSwbQ" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/katex.min.js" integrity="sha384-2BKqo+exmr9su6dir+qCw08N2ZKRucY4PrGQPPWU1A7FtlCGjmEGFqXCv5nyM5Ij" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
    onload="renderMathInElement(document.body,
      {
        delimiters: [
          {left: '$$', right: '$$', display:true},
          {left: '$', right: '$', display:false},
          {left: '\\(', right: '\\)', display: false},
          {left: '\\[', right: '\\]', display: true}
        ]
      }
    );">
  </script>
  </section>

      </div>

      
  <footer class="footer">
    <section class="container">
      
        <p>cbyad</p>
      
      
        ©
        
        2020
         Yannick-Alain Couassi-Blé D. 
      
      
      
        
      
    </section>
  </footer>

    </main>

    

    

    <script>
(function(f, a, t, h, o, m){
	a[h]=a[h]||function(){
		(a[h].q=a[h].q||[]).push(arguments)
	};
	o=f.createElement('script'),
	m=f.getElementsByTagName('script')[0];
	o.async=1; o.src=t; o.id='fathom-script';
	m.parentNode.insertBefore(o,m)
})(document, window, '//analytics.example.com/tracker.js', 'fathom');
fathom('set', 'siteId', 'ABCDE');
fathom('trackPageview');
</script>


  </body>

</html>
